{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Github:** https://github.com/SpikeStriker/Courses/blob/main/5EL272/saavlabs/Lab3/Lab3_Highway_DQN_rlagents%20submission.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kx8X4s8krNWt"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gym\n",
    "import highway_env\n",
    "import os\n",
    "import sys\n",
    "%load_ext tensorboard\n",
    "!git clone https://github.com/eleurent/highway-env.git\n",
    "sys.path.insert(0, os.path.abspath('highway-env/scripts/'))\n",
    "from utils import record_videos,show_videos\n",
    "import imageio_ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r '/uday/out/HighwayEnv/DQNAgent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_agents.trainer.evaluation import Evaluation\n",
    "from rl_agents.agents.common.factory import load_agent, load_environment\n",
    "\n",
    "!git clone https://github.com/eleurent/rl-agents.git\n",
    "env_config = '/uday/rl-agents/scripts/configs/HighwayEnv/env.json'\n",
    "agent_config = '/uday/rl-agents/scripts/configs/HighwayEnv/agents/DQNAgent/dqn.json'\n",
    "import pprint\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global observations:**\n",
    "The agent learns to change lanes more often at lower gamma values leading to a higher number of episodes to learn comparable behavior or reward.\n",
    "The agent learning time however becomes directly proportional to gamma values at higher gamma values when agent tries to pridict higher number of steps in future.\n",
    "\n",
    "A lower temperature leads to lower exploration leading to slower learning with a gready agent.\n",
    "\n",
    "An agent takes higher number of episodes to learn an environment with higher complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tensorboard logs also saved to github repo for reference*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Environment:**\n",
    "\n",
    "*Env 0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*\n",
    "The agent learns quickly to shift lanes to maximise reward and delivers consistent performance across episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = load_environment(env_config)\n",
    "env.config[\"lane_change_reward\"] = 0\n",
    "env.config[\"vehicles_count\"] = 0\n",
    "env.config[\"lanes_count\"] = 2\n",
    "env.config[\"initial_lane_id\"] = 0\n",
    "pprint.pprint(env.config)\n",
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_agent(agent_config, env)\n",
    "agent.config[\"duration\"]=40\n",
    "agent.config[\"gamma\"]=0.6\n",
    "agent.config[\"exploration\"][\"tau\"]=600\n",
    "agent.config[\"exploration\"][\"temperature\"]=0.99\n",
    "agent.config[\"exploration\"][\"final_temperature\"]=0.7\n",
    "agent.config[\"device\"]='cuda:0'\n",
    "# pprint.pprint(agent.config)\n",
    "evaluation = Evaluation(env, agent, num_episodes=5000, display_env=False, run_directory=\"Base_Env0_rightLaneReward\")\n",
    "print(f\"Ready to train {agent} on {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"{evaluation.directory}\"\n",
    "evaluation.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.configure({\"offscreen_rendering\": True})\n",
    "env.render()\n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = Evaluation(env, agent, num_episodes=1000, recover=True)\n",
    "evaluation.test()\n",
    "show_videos(evaluation.run_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding cars to the Environment:**\n",
    "\n",
    "*Env 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*\n",
    "The agent follows a learning curve with gradual increase in length of episode as well as total reward per episode.\n",
    "The agent prefers staying in right lane owing to the right lane reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = load_environment(env_config)\n",
    "env.config[\"lane_change_reward\"] = 0\n",
    "env.config[\"vehicles_count\"] = 10\n",
    "env.config[\"lanes_count\"] = 2\n",
    "env.config[\"initial_lane_id\"] = 0\n",
    "# env.config['reward_speed_range'] = [20, 30]\n",
    "env.config[\"vehicles_density\"]=1\n",
    "\n",
    "pprint.pprint(env.config)\n",
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_agent(agent_config, env)\n",
    "agent.config[\"duration\"]=40\n",
    "agent.config[\"gamma\"]=0.6\n",
    "agent.config[\"exploration\"][\"tau\"]=600\n",
    "agent.config[\"exploration\"][\"temperature\"]=0.99\n",
    "agent.config[\"exploration\"][\"final_temperature\"]=0.7\n",
    "agent.config[\"device\"]='cuda:0'\n",
    "# pprint.pprint(agent.config)\n",
    "evaluation = Evaluation(env, agent, num_episodes=5000, display_env=False, run_directory=\"Env1_withCars\")\n",
    "print(f\"Ready to train {agent} on {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"{evaluation.directory}\"\n",
    "evaluation.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.configure({\"offscreen_rendering\": True})\n",
    "env.render()\n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = Evaluation(env, agent, num_episodes=1000, recover=True)\n",
    "evaluation.test()\n",
    "show_videos(evaluation.run_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding lane change penalty of 0.1 to the Environment: Env 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*\n",
    "The agent follows a learning curve with gradual increase in length of episode as well as total reward per episode.However, both the length and reward remain lower than Env 1 without any lane change panetly.\n",
    "Any unnecessary lane changes are minimal in observed videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = load_environment(env_config)\n",
    "env.config[\"lane_change_reward\"] = -0.1\n",
    "env.config[\"vehicles_count\"] = 10\n",
    "env.config[\"lanes_count\"] = 2\n",
    "env.config[\"initial_lane_id\"] = 0\n",
    "# env.config['reward_speed_range'] = [20, 30]\n",
    "env.config[\"vehicles_density\"]=1\n",
    "\n",
    "pprint.pprint(env.config)\n",
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_agent(agent_config, env)\n",
    "agent.config[\"duration\"]=40\n",
    "agent.config[\"gamma\"]=0.6\n",
    "agent.config[\"exploration\"][\"tau\"]=600\n",
    "agent.config[\"exploration\"][\"temperature\"]=0.99\n",
    "agent.config[\"exploration\"][\"final_temperature\"]=0.7\n",
    "agent.config[\"device\"]='cuda:0'\n",
    "# pprint.pprint(agent.config)\n",
    "evaluation = Evaluation(env, agent, num_episodes=5000, display_env=False, run_directory=\"Env1_laneChangePenalty01\")\n",
    "print(f\"Ready to train {agent} on {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"{evaluation.directory}\"\n",
    "evaluation.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.configure({\"offscreen_rendering\": True})\n",
    "env.render()\n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = Evaluation(env, agent, num_episodes=1000, recover=True)\n",
    "evaluation.test()\n",
    "show_videos(evaluation.run_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Increasing lane change penalty to 1 the Environment: Env 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*\n",
    "The agent follows a learning curve with gradual increase in length of episode as well as total reward per episode.\n",
    "Beacuse of normalisation higher right lane reward is achieved. Hence while length of episode remained similar to Env 1 with 0.1 or no panelty, total reward follows an elevated curve.\n",
    "Since lane change penalty and crash penalty are same, the agent takes either as oppose to slowing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = load_environment(env_config)\n",
    "env.config[\"lane_change_reward\"] = -1\n",
    "env.config[\"vehicles_count\"] = 10\n",
    "env.config[\"lanes_count\"] = 2\n",
    "env.config[\"initial_lane_id\"] = 0\n",
    "# env.config['reward_speed_range'] = [20, 30]\n",
    "env.config[\"vehicles_density\"]=1\n",
    "\n",
    "pprint.pprint(env.config)\n",
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_agent(agent_config, env)\n",
    "agent.config[\"duration\"]=40\n",
    "agent.config[\"gamma\"]=0.6\n",
    "agent.config[\"exploration\"][\"tau\"]=600\n",
    "agent.config[\"exploration\"][\"temperature\"]=0.99\n",
    "agent.config[\"exploration\"][\"final_temperature\"]=0.7\n",
    "agent.config[\"device\"]='cuda:0'\n",
    "# pprint.pprint(agent.config)\n",
    "evaluation = Evaluation(env, agent, num_episodes=5000, display_env=False, run_directory=\"Env1_laneChangePenalty1\")\n",
    "print(f\"Ready to train {agent} on {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"{evaluation.directory}\"\n",
    "evaluation.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.configure({\"offscreen_rendering\": True})\n",
    "env.render()\n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = Evaluation(env, agent, num_episodes=1000, recover=True)\n",
    "evaluation.test()\n",
    "show_videos(evaluation.run_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further increasing lane change penalty to 10 for Environment: Env 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*\n",
    "Since the lane change penalty is much greater than crash penalty, the agent gradually learns to crash early as oppose to drive as observed via a decreasing episode length over time with increasing total reward episode. The videos also show the trend where agent drives to crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = load_environment(env_config)\n",
    "env.config[\"lane_change_reward\"] = -10\n",
    "env.config[\"vehicles_count\"] = 10\n",
    "env.config[\"lanes_count\"] = 2\n",
    "env.config[\"initial_lane_id\"] = 0\n",
    "# env.config['reward_speed_range'] = [20, 30]\n",
    "env.config[\"vehicles_density\"]=1\n",
    "\n",
    "pprint.pprint(env.config)\n",
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_agent(agent_config, env)\n",
    "agent.config[\"duration\"]=40\n",
    "agent.config[\"gamma\"]=0.6\n",
    "agent.config[\"exploration\"][\"tau\"]=600\n",
    "agent.config[\"exploration\"][\"temperature\"]=0.99\n",
    "agent.config[\"exploration\"][\"final_temperature\"]=0.7\n",
    "agent.config[\"device\"]='cuda:0'\n",
    "# pprint.pprint(agent.config)\n",
    "evaluation = Evaluation(env, agent, num_episodes=5000, display_env=False, run_directory=\"Env1_laneChangePenalty10\")\n",
    "print(f\"Ready to train {agent} on {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"{evaluation.directory}\"\n",
    "evaluation.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.configure({\"offscreen_rendering\": True})\n",
    "env.render()\n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = Evaluation(env, agent, num_episodes=1000, recover=True)\n",
    "evaluation.test()\n",
    "show_videos(evaluation.run_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Increasing complexity to ensure collision in Env 1:**\n",
    "*Env 2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*\n",
    "The agent follows a learning curve with gradual increase in length of episode as well as total reward per episode.\n",
    "However these curves remain lower than those for Env 1 with lower complexity and car density. This is also overserved in the vidoes when the agent tries to avoid crashes but also crashes more often than in Env 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = load_environment(env_config)\n",
    "env.config[\"lane_change_reward\"] = 0\n",
    "env.config[\"vehicles_count\"] = 10\n",
    "env.config[\"lanes_count\"] = 2\n",
    "env.config[\"initial_lane_id\"] = 0\n",
    "env.config['reward_speed_range'] = [40, 80]\n",
    "env.config[\"vehicles_density\"]=2\n",
    "\n",
    "pprint.pprint(env.config)\n",
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_agent(agent_config, env)\n",
    "agent.config[\"duration\"]=40\n",
    "agent.config[\"gamma\"]=0.6\n",
    "agent.config[\"exploration\"][\"tau\"]=600\n",
    "agent.config[\"exploration\"][\"temperature\"]=0.99\n",
    "agent.config[\"exploration\"][\"final_temperature\"]=0.7\n",
    "agent.config[\"device\"]='cuda:0'\n",
    "evaluation = Evaluation(env, agent, num_episodes=5000, display_env=False, run_directory=\"Env2_incCarsSpeed\")\n",
    "print(f\"Ready to train {agent} on {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"{evaluation.directory}\"\n",
    "evaluation.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.configure({\"offscreen_rendering\": True})\n",
    "env.render()\n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = Evaluation(env, agent, num_episodes=1000, recover=True)\n",
    "evaluation.test()\n",
    "show_videos(evaluation.run_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Lane Change Panalty of 1 to Env 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:*\n",
    "The agent follows a learning curve with gradual increase in length of episode as well as total reward per episode.\n",
    "Owing to noralisation of penalties and rewards, the right lane reward remains high and consequently the agent returns higher total reward per episode when compared with that of Env. 2 (same complexity but no lane change penalty) even when length of episodes remains similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = load_environment(env_config)\n",
    "env.config[\"lane_change_reward\"] = 0\n",
    "env.config[\"vehicles_count\"] = 10\n",
    "env.config[\"lanes_count\"] = 2\n",
    "env.config[\"initial_lane_id\"] = 0\n",
    "env.config['reward_speed_range'] = [40, 80]\n",
    "env.config[\"vehicles_density\"]=2\n",
    "env.config[\"lane_change_reward\"] = -1\n",
    "\n",
    "\n",
    "pprint.pprint(env.config)\n",
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_agent(agent_config, env)\n",
    "agent.config[\"duration\"]=40\n",
    "agent.config[\"gamma\"]=0.6\n",
    "agent.config[\"exploration\"][\"tau\"]=600\n",
    "agent.config[\"exploration\"][\"temperature\"]=0.99\n",
    "agent.config[\"exploration\"][\"final_temperature\"]=0.7\n",
    "agent.config[\"device\"]='cuda:0'\n",
    "evaluation = Evaluation(env, agent, num_episodes=5000, display_env=False, run_directory=\"Env2_laneChangePenalty1\")\n",
    "print(f\"Ready to train {agent} on {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"{evaluation.directory}\"\n",
    "evaluation.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.configure({\"offscreen_rendering\": True})\n",
    "env.render()\n",
    "agent = load_agent(agent_config, env)\n",
    "evaluation = Evaluation(env, agent, num_episodes=1000, recover=True)\n",
    "evaluation.test()\n",
    "show_videos(evaluation.run_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY0rpVYUtRpN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
